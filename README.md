# Analysis of Music from Text-Conditioned

The field of text-to-music generation presents unique challenges due to the complexity of musical structures and the high fidelity required for realistic audio synthesis. While deep generative models have made strides in text-based content creation, their application to music remains an evolving frontier. This project explores the capabilities of state-of-the-art models—JEN-1, MusicGen, Mustango, and AudioLDM—trained on the MusicCaps and MusicBench datasets to generate music based on textual descriptions. JEN-1, a diffusion-based model, integrates both autoregressive and non-autoregressive training to balance quality, efficiency, and controllability. Mustango, on the other hand, allows for fine-grained control over elements like chords, beats, and tempo, enhancing alignment between textual prompts and generated music.

To assess the effectiveness of these models, we employ a range of evaluation metrics, including CLAP score for text-music alignment, Frechet Audio Distance (FAD) for audio quality, and Kullback-Leibler Divergence (KL) for distribution similarity. Our research highlights the trade-offs between different generation approaches, comparing spectrogram-based and waveform-based methods, as well as autoregressive versus non-autoregressive architectures. By bridging the domains of natural language processing and computational musicology, this study aims to advance high-fidelity text-to-music generation, pushing the boundaries of generative AI in music composition.
